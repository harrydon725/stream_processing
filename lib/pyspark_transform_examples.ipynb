{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b116a11f",
   "metadata": {},
   "source": [
    "# PySpark Transformation Examples\n",
    "\n",
    "This notebook collects compact, runnable examples of the PySpark transformations you've used across the `lib/` files. Each code cell includes explanatory comments describing the purpose, function arguments and common pitfalls.\n",
    "\n",
    "Run cells in order. These are examples intended for learning and reference; adapt paths and Kafka settings to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup: create a Spark session and import frequently used helpers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, regexp_replace, regexp_extract, when, trim, split, explode, from_json, lit, desc)\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count\n",
    "from pyspark.sql.types import MapType, StringType, LongType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Transform Examples\").getOrCreate()\n",
    "# Reduce verbosity when running interactively\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Notes:\n",
    "# - SparkSession: the entry point for DataFrame operations.\n",
    "# - Common functions: col wraps a column name into a Column object; regexp_replace/reg_exp_extract are for text cleaning; when constructs conditional expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9aef1",
   "metadata": {},
   "source": [
    "## Read CSV robustly\n",
    "Example: CSV files often contain quoted fields, embedded commas or newlines. Use these reader options to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b049433",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../spotify-2023.csv\"  # adjust relative path as needed\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")   # use the first row as column names\n",
    "         .option(\"sep\", \",\")          # field separator (comma by default)\n",
    "         .option(\"quote\", '\"')         # character used for quoting fields\n",
    "         .option(\"escape\", '\"')        # escape character for quotes inside fields\n",
    "         .option(\"multiLine\", \"true\") # parse fields spanning multiple lines\n",
    "         .csv(path)\n",
    ")\n",
    "\n",
    "# Quick inspection to verify parsing\n",
    "print(\"Columns:\", df.columns)\n",
    "df.printSchema()\n",
    "df.show(3, truncate=False)\n",
    "\n",
    "# Notes:\n",
    "# - header=True tells Spark to treat the first CSV row as column names.\n",
    "# - multiLine=True is required if some fields contain embedded newlines; it's slower but safer for messy CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d579f",
   "metadata": {},
   "source": [
    "## Cleaning text-based numeric fields (streams example)\n",
    "Many datasets use formatted numbers like `1,234` or `1.2M`. Example below shows removing separators, extracting numeric part and handling K/M/B suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract, when\n",
    "\n",
    "# create an intermediate column streams_str that removes commas/spaces and keeps letters/digits/dot\n",
    "df2 = df.withColumn(\"streams_str\", regexp_replace(col(\"streams\"), \",\", \"\"))\\\n",
    "         .withColumn(\"streams_str\", regexp_replace(col(\"streams_str\"), \" \", \"\"))\\\n",
    "         .withColumn(\"streams_str\", regexp_replace(col(\"streams_str\"), r\"[^0-9A-Za-z.]\", \"\"))\n",
    "\n",
    "# extract numeric part and optional K/M/B suffix\n",
    "df2 = df2.withColumn(\"num_part\", regexp_extract(col(\"streams_str\"), r\"([0-9]*\\.?[0-9]+)\", 1))\\\n",
    "         .withColumn(\"suffix\", regexp_extract(col(\"streams_str\"), r\"([KMB])\", 1))\n",
    "\n",
    "# convert to a numeric value (double) and handle suffix multipliers\n",
    "df2 = df2.withColumn(\"streams_num\",\n",
    "    when(col(\"suffix\")==\"K\", col(\"num_part\").cast(\"double\")*1_000)\n",
    "    .when(col(\"suffix\")==\"M\", col(\"num_part\").cast(\"double\")*1_000_000)\n",
    "    .when(col(\"suffix\")==\"B\", col(\"num_part\").cast(\"double\")*1_000_000_000)\n",
    "    .otherwise(col(\"num_part\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# keep only rows where parsing succeeded and rename the numeric column\n",
    "df_clean = df2.filter(col(\"streams_num\").isNotNull())\\\n",
    "               .withColumnRenamed(\"streams_num\", \"streams\")\\\n",
    "               .drop(\"streams_str\", \"num_part\", \"suffix\")\n",
    "\n",
    "df_clean.select(\"track_name\", \"streams\").show(5, truncate=False)\n",
    "\n",
    "# Notes:\n",
    "# - regexp_replace removes characters by regex (first arg is Column, second is pattern).\n",
    "# - regexp_extract extracts capture groups; the final arg is the group index.\n",
    "# - when / otherwise build conditional expressions (similar to SQL CASE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a98e18",
   "metadata": {},
   "source": [
    "## Aggregations: groupBy + agg\n",
    "Group rows and compute aggregate metrics (sum/count). Common pattern: groupBy(...).agg(sum(col).alias('name'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed11d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: top 5 tracks by total streams (group by track and artist if you want to disambiguate)\n",
    "top5 = (\n",
    "    df_clean.groupBy(\"track_name\", \"artist(s)_name\")\n",
    "           .agg(spark_sum(\"streams\").alias(\"total_streams\"))\n",
    "           .orderBy(desc(\"total_streams\"))\n",
    "           .limit(5)\n",
    ")\n",
    "top5.show(truncate=False)\n",
    "\n",
    "# Notes:\n",
    "# - groupBy accepts one or multiple column names (strings) or Column objects.\n",
    "# - agg takes expressions (e.g., sum, count). alias gives the resulting column a name.\n",
    "# - orderBy(desc('col')) sorts descending; use asc or omit for ascending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e45eb",
   "metadata": {},
   "source": [
    "## Explode / split: handle multi-valued columns (artists example)\n",
    "If a column contains comma-separated artists, `split` followed by `explode` will create one row per artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, trim\n",
    "\n",
    "# create rows per artist and count chart appearances where in_spotify_charts is present\n",
    "df_artists = (\n",
    "    df.withColumn(\"in_spotify_charts_i\", df['in_spotify_charts'].cast(\"int\"))\n",
    "      .filter((col(\"in_spotify_charts_i\") >= 1) & (col(\"in_spotify_charts_i\") <= 1000))\n",
    "      .withColumn(\"artist\", explode(split(col(\"artist(s)_name\"), \",\")))\n",
    "      .withColumn(\"artist\", trim(col(\"artist\")))\n",
    ")\n",
    "artist_counts = df_artists.groupBy(\"artist\").agg(spark_count('*').alias(\"chart_appearances\")).orderBy(desc(\"chart_appearances\"))\n",
    "artist_counts.show(20, truncate=False)\n",
    "\n",
    "# Notes:\n",
    "# - split(column, pattern) returns an Array column; explode(arrayCol) converts each array element into its own row.\n",
    "# - trim removes leading/trailing whitespace which is common after splitting on commas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271da82",
   "metadata": {},
   "source": [
    "## UDFs and from_json (Kafka message parsing)\n",
    "Use `from_json` with a MapType to parse arbitrary JSON payloads produced to Kafka. A small UDF can extract keys from the parsed map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb28e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, udf\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "# Example UDF: safely get a value from a map (declare return type so Spark can optimize)\n",
    "def getKey(d, k):\n",
    "    if d is None:\n",
    "        return None\n",
    "    return d.get(k)\n",
    "\n",
    "udfGetKey = udf(getKey, StringType())\n",
    "\n",
    "# Suppose `kafka_df` is a streaming DataFrame read from Kafka with a binary/string `value` column\n",
    "# kafka_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"orders\").load()\n",
    "# Demonstration (non-streaming): parse the `value` column as a Map and extract named fields\n",
    "demo = df.selectExpr(\"CAST(value AS STRING) as value_str\")\n",
    "demo = demo.withColumn(\"value_map\", from_json(col(\"value_str\"), MapType(StringType(), StringType())))\n",
    "demo = demo.withColumn(\"order_id\", udfGetKey(col(\"value_map\"), lit(\"order_id\")))\n",
    "demo = demo.withColumn(\"amount\", udfGetKey(col(\"value_map\"), lit(\"amount\")))\n",
    "demo.show(5, truncate=False)\n",
    "\n",
    "# Notes:\n",
    "# - from_json(col, schema) parses a JSON string column into the provided schema (MapType is flexible).\n",
    "# - UDFs must declare a return type (e.g., StringType()) when used with Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c93e8f",
   "metadata": {},
   "source": [
    "## Streaming basics (readStream / writeStream)\n",
    "A minimal example showing how to read from Kafka, write to an in-memory table and query it from the driver. Use memory sink only for development/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee19462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka (adjust options for your cluster)\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"localhost:9092\",\n",
    "    \"subscribe\": \"orders\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "kdf = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "# Parse JSON value and write to an in-memory table named 'orders_mem'\n",
    "parsed = kdf.selectExpr(\"CAST(value AS STRING) as value_str\")\n",
    "parsed = parsed.withColumn(\"value_map\", from_json(col(\"value_str\"), MapType(StringType(), StringType())))\n",
    "parsed = parsed.withColumn(\"order_id\", udfGetKey(col(\"value_map\"), lit(\"order_id\")))\n",
    "parsed = parsed.withColumn(\"amount\", udfGetKey(col(\"value_map\"), lit(\"amount\"))).withColumn(\"amount\", col(\"amount\").cast(\"long\"))\n",
    "\n",
    "query = parsed.writeStream.format(\"memory\").queryName(\"orders_mem\").outputMode(\"append\").start()\n",
    "\n",
    "# Notes:\n",
    "# - writeStream.start() returns a StreamingQuery object.\n",
    "# - avoid awaitTermination() in examples where you want the driver to continue executing additional logic.\n",
    "# - memory sink is for testing only (not for production)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659d3ef",
   "metadata": {},
   "source": [
    "## Practical tips and pitfalls\n",
    "- Always inspect `df.printSchema()` after reading external data; mismatched schemas are a common source of NULLs.\n",
    "- Use explicit casts (IntegerType/LongType) when aggregating large numbers to avoid overflow.\n",
    "- Prefer built-in functions over Python UDFs for performance when possible; UDFs break Catalyst optimization.\n",
    "- For large CSVs, specify an explicit schema to avoid expensive inferSchema on big files."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
